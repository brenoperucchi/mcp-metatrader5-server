#!/usr/bin/env python3
"""
Benchmark de Performance - Etapa 2
Valida lat√™ncias P95/P99 do servidor MCP MT5
"""

import asyncio
import time
import statistics
import json
from datetime import datetime
import sys
sys.path.append('../../main_mcp/client')

# Tentar importar o cliente MCP
try:
    from mcp_mt5_client import MT5MCPClient
except ImportError:
    print("‚ö†Ô∏è Cliente MCP n√£o encontrado, usando httpx direto")
    import httpx
    
    class MT5MCPClient:
        def __init__(self, server_url="http://localhost:50051"):
            self.server_url = server_url
            self.client = httpx.AsyncClient(timeout=30.0)
        
        async def __aenter__(self):
            return self
        
        async def __aexit__(self, exc_type, exc_val, exc_tb):
            await self.client.aclose()
        
        async def call_tool(self, tool_name, arguments=None):
            response = await self.client.post(
                f"{self.server_url}/mcp",
                json={
                    "jsonrpc": "2.0",
                    "method": "tools/call",
                    "params": {
                        "name": tool_name,
                        "arguments": arguments or {}
                    },
                    "id": 1
                }
            )
            return response.json()

class PerformanceBenchmark:
    """Benchmark de performance do MCP MT5"""
    
    def __init__(self, server_url="http://localhost:50051"):
        self.server_url = server_url
        self.results = {}
    
    async def measure_operation(self, client, operation_name, tool_name, arguments=None, iterations=60):
        """Mede lat√™ncia de uma opera√ß√£o"""
        print(f"üìä Medindo {operation_name}... ({iterations} itera√ß√µes)")
        
        latencies = []
        errors = 0
        
        for i in range(iterations):
            start = time.perf_counter()
            try:
                result = await client.call_tool(tool_name, arguments)
                elapsed = (time.perf_counter() - start) * 1000  # ms
                latencies.append(elapsed)
                
                # Pequena pausa entre requests
                if i % 10 == 0:
                    print(f"   {i}/{iterations} completo...")
                await asyncio.sleep(0.1)  # 100ms entre requests
                
            except Exception as e:
                errors += 1
                print(f"   ‚ùå Erro: {e}")
        
        if latencies:
            latencies.sort()
            stats = {
                "operation": operation_name,
                "tool": tool_name,
                "iterations": iterations,
                "errors": errors,
                "min": latencies[0],
                "max": latencies[-1],
                "mean": statistics.mean(latencies),
                "median": statistics.median(latencies),
                "stdev": statistics.stdev(latencies) if len(latencies) > 1 else 0,
                "p50": latencies[len(latencies)//2],
                "p95": latencies[int(len(latencies)*0.95)],
                "p99": latencies[min(int(len(latencies)*0.99), len(latencies)-1)],
            }
            
            self.results[operation_name] = stats
            return stats
        
        return None
    
    async def run_benchmark(self):
        """Executa benchmark completo"""
        print("üöÄ Iniciando Benchmark de Performance - Etapa 2")
        print("=" * 60)
        print(f"Servidor: {self.server_url}")
        print(f"Hora in√≠cio: {datetime.now().isoformat()}")
        print("=" * 60)
        
        async with MT5MCPClient(self.server_url) as client:
            # 1. Quotes (get_symbol_info_tick)
            await self.measure_operation(
                client,
                "get_quotes",
                "get_symbol_info_tick",
                {"symbol": "PETR4"},
                iterations=60
            )
            
            # 2. Ticks hist√≥ricos
            await self.measure_operation(
                client,
                "get_ticks",
                "copy_ticks_from_pos",
                {"symbol": "PETR4", "start_pos": 0, "count": 100},
                iterations=30
            )
            
            # 3. Posi√ß√µes
            await self.measure_operation(
                client,
                "get_positions",
                "positions_get",
                {},
                iterations=30
            )
            
            # 4. Ordens
            await self.measure_operation(
                client,
                "get_orders",
                "orders_get",
                {},
                iterations=30
            )
            
            # 5. Valida√ß√£o de ordem (simula place_order)
            order_request = {
                "action": 1,
                "symbol": "PETR4",
                "volume": 100,
                "type": 0,
                "price": 30.00,
                "deviation": 20,
                "magic": 123456,
                "comment": "Benchmark test"
            }
            
            await self.measure_operation(
                client,
                "place_order_check",
                "order_check",
                {"request": order_request},
                iterations=30
            )
        
        # Gerar relat√≥rio
        self.generate_report()
        
        print("\n‚úÖ Benchmark completo!")
        print(f"Hora fim: {datetime.now().isoformat()}")
    
    def generate_report(self):
        """Gera relat√≥rio de benchmark"""
        
        # Salvar resultados raw
        with open("benchmark_results.json", "w") as f:
            json.dump(self.results, f, indent=2)
        
        # Gerar relat√≥rio markdown
        report = f"""# Benchmark Report - Etapa 2

## üìä Resultados de Performance

**Data**: {datetime.now().isoformat()}
**Servidor**: {self.server_url}

## Sum√°rio Executivo

| Opera√ß√£o | Itera√ß√µes | Erros | Min (ms) | P50 (ms) | P95 (ms) | P99 (ms) | Max (ms) |
|----------|-----------|-------|----------|----------|----------|----------|----------|
"""
        
        for op_name, stats in self.results.items():
            report += f"| {op_name} | {stats['iterations']} | {stats['errors']} | "
            report += f"{stats['min']:.2f} | {stats['p50']:.2f} | "
            report += f"{stats['p95']:.2f} | {stats['p99']:.2f} | {stats['max']:.2f} |\n"
        
        # An√°lise de crit√©rios
        report += "\n## ‚úÖ Valida√ß√£o de Crit√©rios\n\n"
        
        # Crit√©rio 1: Quotes P95 < 150ms
        if "get_quotes" in self.results:
            p95_quotes = self.results["get_quotes"]["p95"]
            if p95_quotes < 150:
                report += f"- ‚úÖ **Quotes P95 < 150ms**: {p95_quotes:.2f}ms\n"
            else:
                report += f"- ‚ùå **Quotes P95 < 150ms**: {p95_quotes:.2f}ms (FALHOU)\n"
        
        # Crit√©rio 2: Orders P95 < 400ms
        if "place_order_check" in self.results:
            p95_orders = self.results["place_order_check"]["p95"]
            if p95_orders < 400:
                report += f"- ‚úÖ **Orders P95 < 400ms**: {p95_orders:.2f}ms\n"
            else:
                report += f"- ‚ùå **Orders P95 < 400ms**: {p95_orders:.2f}ms (FALHOU)\n"
        
        # Estat√≠sticas detalhadas
        report += "\n## üìà Estat√≠sticas Detalhadas\n\n"
        
        for op_name, stats in self.results.items():
            report += f"### {op_name}\n\n"
            report += f"- **M√©dia**: {stats['mean']:.2f}ms\n"
            report += f"- **Mediana**: {stats['median']:.2f}ms\n"
            report += f"- **Desvio Padr√£o**: {stats['stdev']:.2f}ms\n"
            report += f"- **Taxa de Erro**: {stats['errors']}/{stats['iterations']} "
            report += f"({100*stats['errors']/stats['iterations']:.1f}%)\n\n"
        
        # Recomenda√ß√µes
        report += "## üéØ Recomenda√ß√µes\n\n"
        
        all_good = True
        if "get_quotes" in self.results and self.results["get_quotes"]["p95"] > 150:
            report += "- ‚ö†Ô∏è Otimizar lat√™ncia de quotes\n"
            all_good = False
        
        if "place_order_check" in self.results and self.results["place_order_check"]["p95"] > 400:
            report += "- ‚ö†Ô∏è Otimizar lat√™ncia de ordens\n"
            all_good = False
        
        for op_name, stats in self.results.items():
            if stats["errors"] > stats["iterations"] * 0.05:  # > 5% erro
                report += f"- ‚ö†Ô∏è Investigar alta taxa de erro em {op_name}\n"
                all_good = False
        
        if all_good:
            report += "- ‚úÖ Performance dentro dos par√¢metros esperados\n"
        
        # Conclus√£o
        report += "\n## üìù Conclus√£o\n\n"
        
        if all_good:
            report += "**Status**: ‚úÖ APROVADO - Servidor atende aos crit√©rios de performance\n"
        else:
            report += "**Status**: ‚ö†Ô∏è APROVADO COM RESSALVAS - Alguns crit√©rios n√£o atendidos\n"
        
        # Salvar relat√≥rio
        with open("benchmark_etapa2.md", "w") as f:
            f.write(report)
        
        print("\nüìÑ Relat√≥rio salvo em: benchmark_etapa2.md")
        print("üìä Dados raw salvos em: benchmark_results.json")

async def main():
    """Fun√ß√£o principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Benchmark MCP MT5 - Etapa 2")
    parser.add_argument("--server", default="http://localhost:50051", help="URL do servidor MCP")
    parser.add_argument("--quick", action="store_true", help="Modo r√°pido (menos itera√ß√µes)")
    
    args = parser.parse_args()
    
    benchmark = PerformanceBenchmark(args.server)
    
    if args.quick:
        print("‚ö° Modo r√°pido ativado (10 itera√ß√µes por opera√ß√£o)")
        # Ajustar para menos itera√ß√µes em modo quick
    
    await benchmark.run_benchmark()

if __name__ == "__main__":
    asyncio.run(main())