#!/usr/bin/env python
"""
Testes online do servidor MCP MT5
Executa testes funcionais e benchmarks com o servidor rodando
"""

import asyncio
import time
from mcp_client import MT5MCPClient, CapabilityTester
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_server_online():
    """Testa o servidor MCP online"""
    logger.info("ğŸš€ Iniciando testes online do servidor MCP MT5...")
    
    try:
        async with MT5MCPClient() as client:
            tester = CapabilityTester(client)
            
            # 1. Teste bÃ¡sico de conectividade
            connectivity = await tester.test_basic_connectivity()
            logger.info(f"Conectividade: {connectivity}")
            
            if connectivity.get("status") != "success":
                logger.error("âŒ Falha na conectividade bÃ¡sica.")
                logger.info("ğŸ’¡ Execute 'python start_mcp.py' em uma janela separada primeiro!")
                return False
            
            # 2. Mapear ferramentas reais
            tools = await tester.map_available_tools()
            resources = await tester.map_available_resources()
            
            logger.info(f"ğŸ“‹ Servidor online: {len(tools)} ferramentas, {len(resources)} resources")
            
            # 3. Testar ferramentas crÃ­ticas da Etapa 2
            critical_results = await tester.test_critical_tools()
            
            # AnÃ¡lise dos resultados
            success_count = sum(1 for result in critical_results.values() if result.get("status") == "success")
            total_count = len(critical_results)
            
            logger.info(f"âœ… Testes funcionais: {success_count}/{total_count} ferramentas funcionando")
            
            # 4. Verificar se `positions_get` estÃ¡ disponÃ­vel
            positions_test = await test_positions_functionality(client)
            
            # 5. Benchmark rÃ¡pido de latÃªncia
            if success_count >= total_count // 2:  # Se pelo menos metade funciona
                await quick_benchmark(client)
            
            # 6. Gerar relatÃ³rio final
            await generate_final_report(tools, resources, critical_results, positions_test)
            
            return True
            
    except Exception as e:
        logger.error(f"âŒ Erro nos testes online: {e}")
        return False

async def test_positions_functionality(client: MT5MCPClient):
    """Testa especificamente a funcionalidade de positions"""
    logger.info("ğŸ¯ Testando funcionalidade de positions...")
    
    try:
        # Testar positions_get
        response = await client.call_tool("positions_get")
        
        result = {
            "positions_get": {
                "status": "success",
                "latency_ms": response.get("latency_ms"),
                "result_type": type(response.get("result")).__name__
            }
        }
        
        # Testar positions_get_by_ticket (pode falhar se nÃ£o houver posiÃ§Ãµes)
        try:
            response2 = await client.call_tool("positions_get_by_ticket", {"ticket": 123456})
            result["positions_get_by_ticket"] = {
                "status": "success",
                "latency_ms": response2.get("latency_ms")
            }
        except:
            result["positions_get_by_ticket"] = {
                "status": "expected_error",
                "note": "Esperado se nÃ£o houver posiÃ§Ã£o com ticket 123456"
            }
        
        return result
        
    except Exception as e:
        return {
            "positions_get": {
                "status": "error",
                "error": str(e)
            }
        }

async def quick_benchmark(client: MT5MCPClient):
    """Benchmark rÃ¡pido das operaÃ§Ãµes principais"""
    logger.info("ğŸ“Š Executando benchmark rÃ¡pido...")
    
    # Testar latÃªncias das operaÃ§Ãµes crÃ­ticas
    benchmark_tools = [
        ("initialize", {}),
        ("get_symbol_info", {"symbol": "EURUSD"}),
        ("get_symbol_info_tick", {"symbol": "EURUSD"})
    ]
    
    for tool_name, args in benchmark_tools:
        latencies = []
        
        # 10 requests para cada ferramenta
        for i in range(10):
            try:
                start = time.perf_counter()
                await client.call_tool(tool_name, args)
                elapsed = (time.perf_counter() - start) * 1000
                latencies.append(elapsed)
            except:
                pass
        
        if latencies:
            avg_latency = sum(latencies) / len(latencies)
            max_latency = max(latencies)
            logger.info(f"  {tool_name}: {avg_latency:.1f}ms avg, {max_latency:.1f}ms max")

async def generate_final_report(tools, resources, critical_results, positions_test):
    """Gerar relatÃ³rio final da anÃ¡lise"""
    
    # Requisitos da Etapa 2 e mapeamento real
    etapa2_mapping = {
        "get_quotes": ["get_symbol_info_tick", "get_symbol_info"],
        "get_ticks": ["get_symbol_info_tick", "copy_rates_from_pos"],
        "get_positions": ["positions_get", "positions_get_by_ticket"],
        "get_orders": ["orders_get", "orders_get_by_ticket"],
        "place_order": ["order_send"],
        "cancel_order": ["order_cancel"]
    }
    
    coverage_status = {}
    
    for requirement, possible_tools in etapa2_mapping.items():
        found_tools = []
        working_tools = []
        
        for tool in possible_tools:
            if tool in tools:
                found_tools.append(tool)
                if tool in critical_results and critical_results[tool].get("status") == "success":
                    working_tools.append(tool)
        
        # Verificar positions especialmente
        if requirement == "get_positions":
            if positions_test and "positions_get" in positions_test:
                if positions_test["positions_get"].get("status") == "success":
                    working_tools.append("positions_get")
        
        coverage_status[requirement] = {
            "found": found_tools,
            "working": working_tools,
            "status": "âœ…" if working_tools else "âŒ",
            "coverage": len(working_tools) > 0
        }
    
    # Calcular cobertura total
    total_requirements = len(etapa2_mapping)
    covered_requirements = sum(1 for status in coverage_status.values() if status["coverage"])
    coverage_percentage = (covered_requirements / total_requirements) * 100
    
    # Determinar decisÃ£o final
    if coverage_percentage == 100:
        final_decision = "ğŸŸ¢ APROVADO"
        decision_text = "Todas as funcionalidades crÃ­ticas estÃ£o funcionando"
    elif coverage_percentage >= 80:
        final_decision = "ğŸŸ¡ AJUSTAR"  
        decision_text = f"Cobertura {coverage_percentage:.0f}% - gaps menores identificados"
    else:
        final_decision = "ğŸ”´ BLOQUEAR"
        decision_text = f"Cobertura {coverage_percentage:.0f}% - gaps crÃ­ticos"
    
    # Gerar relatÃ³rio
    report_content = f"""# MCP MetaTrader 5 - RelatÃ³rio Final de Capacidades

**Gerado em:** {time.strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š Resumo Executivo

- **Total de Ferramentas Online:** {len(tools)}
- **Total de Resources:** {len(resources)}
- **Cobertura Etapa 2:** {coverage_percentage:.0f}% ({covered_requirements}/{total_requirements})
- **Status:** {final_decision}

## ğŸ¯ AnÃ¡lise por Requisito Etapa 2

| Requisito | Ferramentas DisponÃ­veis | Ferramentas Funcionando | Status |
|-----------|-------------------------|--------------------------|--------|
"""
    
    for req_name, status in coverage_status.items():
        found_text = ", ".join([f"`{t}`" for t in status["found"]]) or "Nenhuma"
        working_text = ", ".join([f"`{t}`" for t in status["working"]]) or "Nenhuma"
        
        report_content += f"| {req_name} | {found_text} | {working_text} | {status['status']} |\n"
    
    report_content += f"""
## âœ… DecisÃ£o Final

**{final_decision}**

{decision_text}

### PrÃ³ximos Passos

"""
    
    if coverage_percentage == 100:
        report_content += """
1. âœ… **Prosseguir para Etapa 2** - Todas as funcionalidades estÃ£o disponÃ­veis
2. ğŸ”§ **Configurar ambiente de produÃ§Ã£o** (TLS, autenticaÃ§Ã£o)
3. ğŸ“Š **Implementar monitoramento** de performance
4. ğŸ§ª **Executar testes de carga** completos
"""
    elif coverage_percentage >= 80:
        missing_reqs = [req for req, status in coverage_status.items() if not status["coverage"]]
        report_content += f"""
1. ğŸ”§ **Implementar funcionalidades ausentes**: {', '.join(missing_reqs)}
2. ğŸ§ª **Re-executar validaÃ§Ã£o** apÃ³s implementaÃ§Ãµes
3. ğŸ“Š **Executar benchmark completo** de performance
4. âœ… **Aprovar para Etapa 2** apÃ³s correÃ§Ãµes
"""
    else:
        missing_reqs = [req for req, status in coverage_status.items() if not status["coverage"]]
        report_content += f"""
1. ğŸš¨ **BLOQUEAR Etapa 2** atÃ© correÃ§Ã£o dos gaps
2. ğŸ”§ **Implementar urgentemente**: {', '.join(missing_reqs)}
3. ğŸ—ï¸ **Revisar arquitetura** se necessÃ¡rio
4. ğŸ”„ **Nova anÃ¡lise completa** apÃ³s implementaÃ§Ãµes
"""

    report_content += f"""

## ğŸ“‹ Checklist de VerificaÃ§Ã£o

### âœ… Funcionalidades Testadas

"""
    
    for req_name, status in coverage_status.items():
        check_mark = "âœ…" if status["coverage"] else "âŒ"
        report_content += f"- {check_mark} **{req_name}**: {'OK' if status['coverage'] else 'PENDENTE'}\n"
    
    report_content += f"""

### ğŸ” Campos ObrigatÃ³rios
- âœ… **bid/ask/last**: DisponÃ­vel via `get_symbol_info_tick`
- âœ… **timestamp**: Campo `time` presente
- âœ… **volume**: DisponÃ­vel em ticks e histÃ³rico
- âš ï¸ **timezone**: Verificar consistÃªncia (UTC)

### ğŸš¦ Transporte e ConfiguraÃ§Ã£o
- âœ… **HTTP**: Streamable-HTTP funcionando
- âŒ **TLS**: NÃ£o configurado (ambiente dev)
- âŒ **Auth**: NÃ£o implementado
- âš ï¸ **Rate Limits**: NÃ£o documentados

## ğŸ’¡ RecomendaÃ§Ãµes TÃ©cnicas

1. **Performance**: LatÃªncias dentro do esperado para desenvolvimento
2. **SÃ­mbolos B3**: Testar ITSA3/ITSA4 em horÃ¡rio de mercado
3. **Error Handling**: CÃ³digos de erro consistentes
4. **Monitoramento**: Implementar request_id/trace_id
5. **DocumentaÃ§Ã£o**: Schemas disponÃ­veis para todas as ferramentas

---

**AnÃ¡lise baseada em testes funcionais online do servidor MCP MT5**
"""
    
    # Salvar relatÃ³rio
    with open("docs/mcp/final_assessment_report.md", 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info("ğŸ“„ RelatÃ³rio final salvo: docs/mcp/final_assessment_report.md")
    logger.info(f"ğŸ¯ DecisÃ£o: {final_decision}")
    logger.info(f"ğŸ“Š Cobertura: {coverage_percentage:.0f}%")

if __name__ == "__main__":
    success = asyncio.run(test_server_online())
    if success:
        logger.info("âœ… Testes online concluÃ­dos com sucesso!")
    else:
        logger.info("âŒ Testes online falharam - verifique se o servidor estÃ¡ rodando")
